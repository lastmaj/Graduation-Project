{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "split train dev test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IbP4Dl40fZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYqBipMv0olU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import a file from drive : \n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW7azK500vqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1g2BefhpdVbg6aeFcngD52p_HLza6nQDm'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('IMDB Dataset.csv')  \n",
        "imdb = pd.read_csv('IMDB Dataset.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBakykoJ1nDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = imdb[\"review\"]\n",
        "y = [1 if word==\"positive\" else 0 for word in imdb[\"sentiment\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j21B9aRS4Qpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "  #standarize text to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  #remove html tags\n",
        "  remove_tags = re.compile('<.*?>')\n",
        "  text= re.sub(remove_tags, '', text)\n",
        "\n",
        "  #remove special characters and numbers\n",
        "  remove_special_char = re.compile('[^a-z ]', re.IGNORECASE)\n",
        "  text = remove_special_char.sub('', text)\n",
        "\n",
        "  #expand decontractions\n",
        "  text = contractions.fix(text)\n",
        "\n",
        "  #setting stopwords\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "\n",
        "  #for every word in the review, if it's not a stopword, replace it with it's lemma, otherwise go for next word\n",
        "  processed_text = []\n",
        "  text = text.split()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  for word in text:\n",
        "    if word in stop_words:\n",
        "      continue\n",
        "      \n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    word = lemmatizer.lemmatize(word, 'v')\n",
        "    processed_text.append(word)\n",
        "\n",
        "  text = ' '.join(processed_text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs0TkK5a4yEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x = []\n",
        "\n",
        "for review in list(imdb['review']):\n",
        "  clean = preprocess_text(review)\n",
        "  x.append(clean)\n",
        "  if (x.index(clean) % 1000 == 0):\n",
        "    print(x.index(clean))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zURiNK1644bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "921e7721-64ed-44b6-fd4d-653dbd52b4b6"
      },
      "source": [
        "from keras.preprocessing import sequence \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "y = [1 if sentiment=='positive' else 0 for sentiment in list(imdb['sentiment'])]\n",
        "tokenizer = Tokenizer(num_words=50000)\n",
        "tokenizer.fit_on_texts(x)\n",
        "x = pad_sequences(tokenizer.texts_to_sequences(x), maxlen=100)\n",
        "max_words = 100\n",
        "vocabulary_size = 50000\n",
        "\n",
        "# Build the model\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "embedding_size=32\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 32)           1600000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,653,301\n",
            "Trainable params: 1,653,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0on_xJB5iu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JLiHsgR5x0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJDghfhP6HHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "f85cb581-5de7-474c-892e-30dd72665e23"
      },
      "source": [
        "batchSize = 64\n",
        "epochs = 3\n",
        "hist = model.fit(xTrain, yTrain, batch_size=batchSize, epochs=epochs, verbose=1, shuffle=True, validation_split=0.1)\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 36000 samples, validate on 4000 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "36000/36000 [==============================] - 95s 3ms/step - loss: 0.3625 - acc: 0.8321 - val_loss: 0.3097 - val_acc: 0.8770\n",
            "Epoch 2/3\n",
            "36000/36000 [==============================] - 94s 3ms/step - loss: 0.1968 - acc: 0.9269 - val_loss: 0.3020 - val_acc: 0.8748\n",
            "Epoch 3/3\n",
            "36000/36000 [==============================] - 95s 3ms/step - loss: 0.1205 - acc: 0.9567 - val_loss: 0.3698 - val_acc: 0.8690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coPnQXsO6W5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d8a72658-3859-4cc0-d2f0-735ba044771e"
      },
      "source": [
        "scores = model.evaluate(xTest, yTest, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-3q1EE7lpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "68dac16f-9bab-4af0-dbd5-65357fc39558"
      },
      "source": [
        "predicted = [\"I advise you not to repeat this again\"]\n",
        "predicted_tokenized = pad_sequences(tokenizer.texts_to_sequences(predicted), maxlen=100)\n",
        "model.predict_classes(predicted_tokenized)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2N8_dPV8OTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}