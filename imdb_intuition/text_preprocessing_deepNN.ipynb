{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrdKBz4gDhwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAM2koo3DpDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html(text):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  return re.sub(cleanr, '', text)\n",
        "\n",
        "def decontract(text):\n",
        "  return contractions.fix(text)\n",
        "\n",
        "def tokenize(text):\n",
        "  # makes text lowercase, removes all non-alphabetic chars and tokenizes it\n",
        "  words = word_tokenize(text.lower())\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  return words\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  words = [word for word in text if not word in stop_words]\n",
        "  return words\n",
        "\n",
        "def lemmatize(text):\n",
        "  # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "  nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "  text = ' '.join(text)  \n",
        "  doc = nlp(text)\n",
        "  lemmas = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc]\n",
        "  return lemmas\n",
        "\n",
        "def text_preprocessing(text):\n",
        "  html = remove_html(text)\n",
        "  decontracted = decontract(html)\n",
        "  tokenized = tokenize(decontracted)\n",
        "  stopword = remove_stopwords(tokenized)\n",
        "  lemmatized = lemmatize(stopword)\n",
        "  clean = ' '.join(lemmatized)\n",
        "  return clean\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3SdkuHP8ZAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AydR2zH9U-nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-mHZfsJVISN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "  #standarize text to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  #remove html tags\n",
        "  remove_tags = re.compile('<.*?>')\n",
        "  text= re.sub(remove_tags, '', text)\n",
        "\n",
        "  #remove special characters and numbers\n",
        "  remove_special_char = re.compile('[^a-z ]', re.IGNORECASE)\n",
        "  text = remove_special_char.sub('', text)\n",
        "\n",
        "  #expand decontractions\n",
        "  text = contractions.fix(text)\n",
        "\n",
        "  #setting stopwords\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "\n",
        "  #for every word in the review, if it's not a stopword, replace it with it's lemma, otherwise go for next word\n",
        "  processed_text = []\n",
        "  text = text.split()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  for word in text:\n",
        "    if word in stop_words:\n",
        "      continue\n",
        "      \n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    word = lemmatizer.lemmatize(word, 'v')\n",
        "    processed_text.append(word)\n",
        "\n",
        "  text = ' '.join(processed_text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz-Gz5n0MuS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess_text(\"for the best lemmas ever, i don't know which one to use\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPi5xc6iQLVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "imdb = pd.read_csv(io.BytesIO(uploaded['IMDB Dataset.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RCPYPGldZSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x = []\n",
        "\n",
        "for review in list(imdb['review']):\n",
        "  clean = preprocess_text(review)\n",
        "  x.append(clean)\n",
        "  if (x.index(clean) % 1000 == 0):\n",
        "    print(x.index(clean))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaJQYn6ce2AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = [1 if sentiment=='positive' else 0 for sentiment in list(imdb['sentiment'])]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu-hOFYWiVNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5c1tFTmihVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokenized = pad_sequences(tokenizer.texts_to_sequences(x), maxlen=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNUnX1t0ikd5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "6f2e7631-3cd0-4896-d6e5-28a5f1605719"
      },
      "source": [
        "print(x[49999], y[49999])\n",
        "print(x_tokenized[49999])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one expect star trek movie high art fan expect movie good best episode unfortunately movie muddle implausible plot leave cringe far worst nine far movie even chance watch well know character interact another movie save movie include goofy scene kirk spock mccoy yosemitei would say movie worth rental hardly worth watch however true fan need see movie rent movie way see even cable channel avoid movie 0\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    3  170   82 1711    1  235\n",
            "  345  114  170    1    9   53  177  414    1 3682 3171   47   99 2432\n",
            "  141  162 2657  141    1   14  424   13   22   24   11 3465   76    1\n",
            "  279    1  234 2296   19 3326 3898   10   21    1  185 1860  842  185\n",
            "   13  116  197  114  112    6    1  393    1   38    6   14 1633  838\n",
            "  570    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wMtuBmvi9Qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words = 100\n",
        "vocabulary_size = 5000\n",
        "\n",
        "# Build the model\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "embedding_size=32\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH1ByvTJjf3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "a159d5c0-5198-479a-ac3f-c98c3dc13945"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCPhAm7_jtLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "b4245880-f276-4678-92d8-254dcc43a8c5"
      },
      "source": [
        "batchSize = 64\n",
        "epochs = 3\n",
        "hist = model.fit(x_tokenized, y, batch_size=batchSize, epochs=epochs, verbose=1, shuffle=True, validation_split=0.5)\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 70s 3ms/step - loss: 0.3125 - acc: 0.8722 - val_loss: 0.3070 - val_acc: 0.8703\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 72s 3ms/step - loss: 0.2537 - acc: 0.9003 - val_loss: 0.3348 - val_acc: 0.8665\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 72s 3ms/step - loss: 0.2259 - acc: 0.9118 - val_loss: 0.3314 - val_acc: 0.8658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJNOCqvFn-_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1fe0df7c-4864-48a8-f5e2-8d82ffbdd8da"
      },
      "source": [
        "predicted = [\"This is very horrible, I can't believe I bought this service\"]\n",
        "predicted_tokenized = pad_sequences(tokenizer.texts_to_sequences(predicted), maxlen=100)\n",
        "model.predict_classes(predicted_tokenized)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WruoRl6yoHju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "67c6de8a-e113-49bc-9f54-548ac407076a"
      },
      "source": [
        "predicted = [\"Although I have some doubts, I would be excited to try this when it comes out\"]\n",
        "predicted_tokenized = pad_sequences(tokenizer.texts_to_sequences(predicted), maxlen=100)\n",
        "model.predict_classes(predicted_tokenized)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DEcus3roS9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bdff2aa4-bf8e-44e5-e76c-8518bdb62d6e"
      },
      "source": [
        "predicted = [\"To be honest, one of the rarest and best services out there\"]\n",
        "predicted_tokenized = pad_sequences(tokenizer.texts_to_sequences(predicted), maxlen=100)\n",
        "model.predict_classes(predicted_tokenized)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ7cie6roYv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}