{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrdKBz4gDhwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAM2koo3DpDD",
        "colab_type": "code",
        "outputId": "36b96bb0-a851-48eb-d39b-3d904d5a010f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_html(text):\n",
        "  cleanr = re.compile('<.*?>') \n",
        "  return re.sub(cleanr, '', text)\n",
        "\n",
        "def decontract(text):\n",
        "  return contractions.fix(text)\n",
        "\n",
        "def tokenize(text):\n",
        "  # makes text lowercase, removes all non-alphabetic chars and tokenizes it\n",
        "  words = word_tokenize(text.lower())\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  return words\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  words = [word for word in text if not word in stop_words]\n",
        "  return words\n",
        "\n",
        "def lemmatize(text):\n",
        "  # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "  nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "  text = ' '.join(text)  \n",
        "  doc = nlp(text)\n",
        "  lemmas = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc]\n",
        "  return lemmas\n",
        "\n",
        "text = \" I once told you y'all full of bullshit. Worst service ever <br> <emo>\"\n",
        "html = remove_html(text)\n",
        "decontracted = decontract(html)\n",
        "tokenized = tokenize(decontracted)\n",
        "stopword = remove_stopwords(tokenized)\n",
        "lemmatized = lemmatize(stopword)\n",
        "\n",
        "print(lemmatized)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tell', 'full', 'bullshit', 'bad', 'service', 'ever']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3SdkuHP8ZAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}