{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd sentment analysis-imdb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYcUb63hmZQo",
        "colab_type": "text"
      },
      "source": [
        "# Improving the first iteration\n",
        "The first iteration was to get a feel of Keras, NLP and datasets.\n",
        "The second iteration will use the same dataset, but will include data preprocessing and a more sophisticated architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPrkWT9ynPTo",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg98zlJ7qadW",
        "colab_type": "text"
      },
      "source": [
        "## Removing html tags\n",
        "An alternative solution is to use a package called \"BeautifulSoup\". This solution is based on a regex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA7Oh7ZBqqgY",
        "colab_type": "code",
        "outputId": "016bd015-ad3b-4446-c2ad-e76e3a2e663c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "cleanhtml(\"An our code <br/> does not contain any <strong>html</strong> tags anymore\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An our code  does not contain any html tags anymore'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX_sPVinsi49",
        "colab_type": "text"
      },
      "source": [
        "## Convert accented characters (can be omitted for now)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxYS3yl-ssxb",
        "colab_type": "text"
      },
      "source": [
        "## Expand contractions\n",
        "Contractions are shortened words (ex: won't, can't, aren't ... ). To help standarize our input, it is better to expand those contractions to their complete form (ex: will not, can not).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu8_Z9S9qHFe",
        "colab_type": "text"
      },
      "source": [
        "We will rely on a package to expand contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHBZXWnwshG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ7jwSWrxVuw",
        "colab_type": "code",
        "outputId": "8e8325e8-5ff9-4f93-9658-93c9554e7ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "contractions.fix(\"y'all happy now\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you all happy now'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOJ5PUOq8HCH",
        "colab_type": "text"
      },
      "source": [
        "## Removing numbers\n",
        "Since numbers do not convey any useful information concerning sentiment analysis, we can remove them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqHPAt1m8bED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9931119-ef59-4bfb-8196-292dfe6d43bc"
      },
      "source": [
        " text = \"3 of your finest ten wines\"\n",
        "replace_numerics = re.compile(r'\\d+', re.IGNORECASE)\n",
        "\n",
        " text = replace_numerics.sub('', text)\n",
        " print(text)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " of your finest ten wines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UcoXGCE-Twn",
        "colab_type": "text"
      },
      "source": [
        "## Removing stop words\n",
        "Stop words refer to words used commonly, but filtered out before lanuguage processing. Examples include 'the', 'in', 'on', 'who', ....\n",
        "\n",
        "We will use nltk (natural language toolkit) to perform this task\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcBTGKDD-7Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS1nSvip_DGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68d53ed2-4c6b-4af6-fc13-87bc62128693"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "example_sent = \"the process of removing the stop words is a crucial step of preprocessing\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "print(word_tokens) \n",
        "print(filtered_sentence) \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'process', 'of', 'removing', 'the', 'stop', 'words', 'is', 'a', 'crucial', 'step', 'of', 'preprocessing']\n",
            "['process', 'removing', 'stop', 'words', 'crucial', 'step', 'preprocessing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pfrlPizGI1U",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization\n",
        "Lemmatization is the process of converting a word to its base form. \n",
        "(ex: better => good)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f10f9gBjH37k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cc257a3-e82c-490e-fd73-4bc6f7404f3f"
      },
      "source": [
        "import spacy\n",
        "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "doc = nlp(text)\n",
        "mytokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc]\n",
        "print(mytokens) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'stripe', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWousulqH8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}